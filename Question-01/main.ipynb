{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dae118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0691e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… punkt already downloaded\n",
      "âœ… stopwords already downloaded\n",
      "ğŸ“¥ Downloading wordnet...\n",
      "âœ… wordnet downloaded successfully\n",
      "âœ… averaged_perceptron_tagger already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "def download_nltk_data():\n",
    "    \"\"\"Download necessary NLTK data packages with error handling\"\"\"\n",
    "    packages = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger']\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{package}' if package == 'punkt' \n",
    "                          else f'taggers/{package}' if package == 'averaged_perceptron_tagger'\n",
    "                          else f'corpora/{package}')\n",
    "            print(f\"âœ… {package} already downloaded\")\n",
    "        except LookupError:\n",
    "            print(f\"ğŸ“¥ Downloading {package}...\")\n",
    "            nltk.download(package, quiet=True)\n",
    "            print(f\"âœ… {package} downloaded successfully\")\n",
    "\n",
    "# Download the data\n",
    "download_nltk_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d5eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Original Text:\n",
      "================================================================================\n",
      "Imagine each paragraph as a sandwich. The real content of the sandwichâ€”the meat or other fillingâ€”is in the middle. It includes all the evidence you need to make the point. But it gets kind of messy to eat a sandwich without any bread. Your readers donâ€™t know what to do with all the evidence youâ€™ve given them. So, the top slice of bread (the first sentence of the paragraph) explains the topic (or controlling idea) of the paragraph. And, the bottom slice (the last sentence of the paragraph) tells the reader how the paragraph relates to the broader argument. In the original and revised paragraphs below, notice how a topic sentence expressing the controlling idea tells the reader the point of all the evidence.\n",
      "================================================================================\n",
      "ğŸ“Š Text Statistics:\n",
      "   - Characters: 715\n",
      "   - Characters (no spaces): 592\n",
      "   - Lines: 1\n"
     ]
    }
   ],
   "source": [
    "# Read paragraph from file\n",
    "def read_paragraph(filename):\n",
    "    \"\"\"Read paragraph from file with error handling\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "\n",
    "# Read the text\n",
    "filename = \"paragraph.txt\"\n",
    "original_text = read_paragraph(filename)\n",
    "\n",
    "if original_text:\n",
    "    print(\"ğŸ“– Original Text:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(original_text)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ“Š Text Statistics:\")\n",
    "    print(f\"   - Characters: {len(original_text)}\")\n",
    "    print(f\"   - Characters (no spaces): {len(original_text.replace(' ', ''))}\")\n",
    "    print(f\"   - Lines: {original_text.count(chr(10)) + 1}\")\n",
    "else:\n",
    "    print(\"âŒ Could not read the text file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06f181a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ TOKENIZATION RESULTS:\n",
      "==================================================\n",
      "ğŸ“Š Total tokens: 145\n",
      "ğŸ”— First 20 tokens: ['Imagine', 'each', 'paragraph', 'as', 'a', 'sandwich', '.', 'The', 'real', 'content', 'of', 'the', 'sandwichâ€”the', 'meat', 'or', 'other', 'fillingâ€”is', 'in', 'the', 'middle']\n",
      "ğŸ”— Last 10 tokens: ['tells', 'the', 'reader', 'the', 'point', 'of', 'all', 'the', 'evidence', '.']\n",
      "\n",
      "ğŸ“ All tokens:\n",
      "'Imagine'  'each'  'paragraph'  'as'  'a'  'sandwich'  '.'  'The'  'real'  'content'  \n",
      "'of'  'the'  'sandwichâ€”the'  'meat'  'or'  'other'  'fillingâ€”is'  'in'  'the'  'middle'  \n",
      "'.'  'It'  'includes'  'all'  'the'  'evidence'  'you'  'need'  'to'  'make'  \n",
      "'the'  'point'  '.'  'But'  'it'  'gets'  'kind'  'of'  'messy'  'to'  \n",
      "'eat'  'a'  'sandwich'  'without'  'any'  'bread'  '.'  'Your'  'readers'  'don'  \n",
      "'â€™'  't'  'know'  'what'  'to'  'do'  'with'  'all'  'the'  'evidence'  \n",
      "'you'  'â€™'  've'  'given'  'them'  '.'  'So'  ','  'the'  'top'  \n",
      "'slice'  'of'  'bread'  '('  'the'  'first'  'sentence'  'of'  'the'  'paragraph'  \n",
      "')'  'explains'  'the'  'topic'  '('  'or'  'controlling'  'idea'  ')'  'of'  \n",
      "'the'  'paragraph'  '.'  'And'  ','  'the'  'bottom'  'slice'  '('  'the'  \n",
      "'last'  'sentence'  'of'  'the'  'paragraph'  ')'  'tells'  'the'  'reader'  'how'  \n",
      "'the'  'paragraph'  'relates'  'to'  'the'  'broader'  'argument'  '.'  'In'  'the'  \n",
      "'original'  'and'  'revised'  'paragraphs'  'below'  ','  'notice'  'how'  'a'  'topic'  \n",
      "'sentence'  'expressing'  'the'  'controlling'  'idea'  'tells'  'the'  'reader'  'the'  'point'  \n",
      "'of'  'all'  'the'  'evidence'  '.'  \n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text into words using NLTK\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "if original_text:\n",
    "    # Perform tokenization\n",
    "    tokens = tokenize_text(original_text)\n",
    "    \n",
    "    print(\"ğŸ”¤ TOKENIZATION RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“Š Total tokens: {len(tokens)}\")\n",
    "    print(f\"ğŸ”— First 20 tokens: {tokens[:20]}\")\n",
    "    print(f\"ğŸ”— Last 10 tokens: {tokens[-10:]}\")\n",
    "    \n",
    "    # Show all tokens in a more readable format\n",
    "    print(f\"\\nğŸ“ All tokens:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"'{token}'\", end=\"  \")\n",
    "        if (i + 1) % 10 == 0:  # New line every 10 tokens\n",
    "            print()\n",
    "    print()  # Final newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82621410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ TOKEN CLEANING RESULTS:\n",
      "==================================================\n",
      "ğŸ“Š Original tokens: 145\n",
      "ğŸ“Š Cleaned tokens: 126\n",
      "ğŸ“Š Tokens removed: 19\n",
      "\n",
      "ğŸ” Comparison of original vs cleaned tokens (first 15):\n",
      "Original        Cleaned        \n",
      "------------------------------\n",
      "Imagine         imagine        \n",
      "each            each           \n",
      "paragraph       paragraph      \n",
      "as              as             \n",
      "a               a              \n",
      "sandwich        sandwich       \n",
      ".               the            \n",
      "The             real           \n",
      "real            content        \n",
      "content         of             \n",
      "of              the            \n",
      "the             sandwichthe    \n",
      "sandwichâ€”the    meat           \n",
      "meat            or             \n",
      "or              other          \n",
      "\n",
      "ğŸ“ All cleaned tokens:\n",
      "'imagine'  'each'  'paragraph'  'as'  'a'  'sandwich'  'the'  'real'  'content'  'of'  'the'  'sandwichthe'  \n",
      "'meat'  'or'  'other'  'fillingis'  'in'  'the'  'middle'  'it'  'includes'  'all'  'the'  'evidence'  \n",
      "'you'  'need'  'to'  'make'  'the'  'point'  'but'  'it'  'gets'  'kind'  'of'  'messy'  \n",
      "'to'  'eat'  'a'  'sandwich'  'without'  'any'  'bread'  'your'  'readers'  'don'  't'  'know'  \n",
      "'what'  'to'  'do'  'with'  'all'  'the'  'evidence'  'you'  've'  'given'  'them'  'so'  \n",
      "'the'  'top'  'slice'  'of'  'bread'  'the'  'first'  'sentence'  'of'  'the'  'paragraph'  'explains'  \n",
      "'the'  'topic'  'or'  'controlling'  'idea'  'of'  'the'  'paragraph'  'and'  'the'  'bottom'  'slice'  \n",
      "'the'  'last'  'sentence'  'of'  'the'  'paragraph'  'tells'  'the'  'reader'  'how'  'the'  'paragraph'  \n",
      "'relates'  'to'  'the'  'broader'  'argument'  'in'  'the'  'original'  'and'  'revised'  'paragraphs'  'below'  \n",
      "'notice'  'how'  'a'  'topic'  'sentence'  'expressing'  'the'  'controlling'  'idea'  'tells'  'the'  'reader'  \n",
      "'the'  'point'  'of'  'all'  'the'  'evidence'  \n"
     ]
    }
   ],
   "source": [
    "# Clean and normalize tokens\n",
    "def clean_and_normalize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Clean tokens using regex to keep only ASCII alphabets and numbers\n",
    "    Convert to lowercase, trim, and remove empty tokens\n",
    "    \"\"\"\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Convert to lowercase\n",
    "        token_lower = token.lower()\n",
    "        \n",
    "        # Use regex to keep only ASCII letters and numbers\n",
    "        # This will preserve words like \"covid19\" instead of losing them\n",
    "        cleaned_token = re.sub(r'[^a-zA-Z0-9]', '', token_lower)\n",
    "        \n",
    "        # Trim whitespace (though regex above should handle this)\n",
    "        cleaned_token = cleaned_token.strip()\n",
    "        \n",
    "        # Only keep non-empty tokens\n",
    "        if len(cleaned_token) > 0:\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "if original_text:\n",
    "    # Clean the tokens\n",
    "    cleaned_tokens = clean_and_normalize_tokens(tokens)\n",
    "    \n",
    "    print(\"ğŸ§¹ TOKEN CLEANING RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“Š Original tokens: {len(tokens)}\")\n",
    "    print(f\"ğŸ“Š Cleaned tokens: {len(cleaned_tokens)}\")\n",
    "    print(f\"ğŸ“Š Tokens removed: {len(tokens) - len(cleaned_tokens)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” Comparison of original vs cleaned tokens (first 15):\")\n",
    "    print(f\"{'Original':<15} {'Cleaned':<15}\")\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(min(15, len(tokens))):\n",
    "        original = tokens[i] if i < len(tokens) else \"\"\n",
    "        cleaned = cleaned_tokens[i] if i < len(cleaned_tokens) else \"\"\n",
    "        print(f\"{original:<15} {cleaned:<15}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ All cleaned tokens:\")\n",
    "    for i, token in enumerate(cleaned_tokens):\n",
    "        print(f\"'{token}'\", end=\"  \")\n",
    "        if (i + 1) % 12 == 0:  # New line every 12 tokens\n",
    "            print()\n",
    "    print()  # Final newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e953e30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ STEMMING RESULTS:\n",
      "============================================================\n",
      "ğŸ“Š Words processed: 126\n",
      "\n",
      "ğŸ” Comparison of stemming methods (first 20 words):\n",
      "Original        Porter          Snowball       \n",
      "---------------------------------------------\n",
      "imagine         imagin          imagin         \n",
      "each            each            each           \n",
      "paragraph       paragraph       paragraph      \n",
      "as              as              as             \n",
      "a               a               a              \n",
      "sandwich        sandwich        sandwich       \n",
      "the             the             the            \n",
      "real            real            real           \n",
      "content         content         content        \n",
      "of              of              of             \n",
      "the             the             the            \n",
      "sandwichthe     sandwichth      sandwichth     \n",
      "meat            meat            meat           \n",
      "or              or              or             \n",
      "other           other           other          \n",
      "fillingis       fillingi        fillingi       \n",
      "in              in              in             \n",
      "the             the             the            \n",
      "middle          middl           middl          \n",
      "it              it              it             \n",
      "\n",
      "ğŸ“ Porter Stemmed words:\n",
      "'imagin'  'each'  'paragraph'  'as'  'a'  'sandwich'  'the'  'real'  'content'  'of'  'the'  'sandwichth'  \n",
      "'meat'  'or'  'other'  'fillingi'  'in'  'the'  'middl'  'it'  'includ'  'all'  'the'  'evid'  \n",
      "'you'  'need'  'to'  'make'  'the'  'point'  'but'  'it'  'get'  'kind'  'of'  'messi'  \n",
      "'to'  'eat'  'a'  'sandwich'  'without'  'ani'  'bread'  'your'  'reader'  'don'  't'  'know'  \n",
      "'what'  'to'  'do'  'with'  'all'  'the'  'evid'  'you'  've'  'given'  'them'  'so'  \n",
      "'the'  'top'  'slice'  'of'  'bread'  'the'  'first'  'sentenc'  'of'  'the'  'paragraph'  'explain'  \n",
      "'the'  'topic'  'or'  'control'  'idea'  'of'  'the'  'paragraph'  'and'  'the'  'bottom'  'slice'  \n",
      "'the'  'last'  'sentenc'  'of'  'the'  'paragraph'  'tell'  'the'  'reader'  'how'  'the'  'paragraph'  \n",
      "'relat'  'to'  'the'  'broader'  'argument'  'in'  'the'  'origin'  'and'  'revis'  'paragraph'  'below'  \n",
      "'notic'  'how'  'a'  'topic'  'sentenc'  'express'  'the'  'control'  'idea'  'tell'  'the'  'reader'  \n",
      "'the'  'point'  'of'  'all'  'the'  'evid'  \n",
      "\n",
      "ğŸ“ Snowball Stemmed words:\n",
      "'imagin'  'each'  'paragraph'  'as'  'a'  'sandwich'  'the'  'real'  'content'  'of'  'the'  'sandwichth'  \n",
      "'meat'  'or'  'other'  'fillingi'  'in'  'the'  'middl'  'it'  'includ'  'all'  'the'  'evid'  \n",
      "'you'  'need'  'to'  'make'  'the'  'point'  'but'  'it'  'get'  'kind'  'of'  'messi'  \n",
      "'to'  'eat'  'a'  'sandwich'  'without'  'ani'  'bread'  'your'  'reader'  'don'  't'  'know'  \n",
      "'what'  'to'  'do'  'with'  'all'  'the'  'evid'  'you'  've'  'given'  'them'  'so'  \n",
      "'the'  'top'  'slice'  'of'  'bread'  'the'  'first'  'sentenc'  'of'  'the'  'paragraph'  'explain'  \n",
      "'the'  'topic'  'or'  'control'  'idea'  'of'  'the'  'paragraph'  'and'  'the'  'bottom'  'slice'  \n",
      "'the'  'last'  'sentenc'  'of'  'the'  'paragraph'  'tell'  'the'  'reader'  'how'  'the'  'paragraph'  \n",
      "'relat'  'to'  'the'  'broader'  'argument'  'in'  'the'  'origin'  'and'  'revis'  'paragraph'  'below'  \n",
      "'notic'  'how'  'a'  'topic'  'sentenc'  'express'  'the'  'control'  'idea'  'tell'  'the'  'reader'  \n",
      "'the'  'point'  'of'  'all'  'the'  'evid'  \n"
     ]
    }
   ],
   "source": [
    "# Perform stemming\n",
    "def perform_stemming(tokens):\n",
    "    \"\"\"Apply both Porter and Snowball stemmers\"\"\"\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    snowball_stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    porter_stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "    snowball_stems = [snowball_stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return porter_stems, snowball_stems\n",
    "\n",
    "if original_text:\n",
    "    # Perform stemming\n",
    "    porter_stemmed, snowball_stemmed = perform_stemming(cleaned_tokens)\n",
    "    \n",
    "    print(\"âœ‚ï¸ STEMMING RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“Š Words processed: {len(cleaned_tokens)}\")\n",
    "    \n",
    "    # Compare stemming results\n",
    "    print(f\"\\nğŸ” Comparison of stemming methods (first 20 words):\")\n",
    "    print(f\"{'Original':<15} {'Porter':<15} {'Snowball':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    for i in range(min(20, len(cleaned_tokens))):\n",
    "        original = cleaned_tokens[i]\n",
    "        porter = porter_stemmed[i]\n",
    "        snowball = snowball_stemmed[i]\n",
    "        print(f\"{original:<15} {porter:<15} {snowball:<15}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Porter Stemmed words:\")\n",
    "    for i, word in enumerate(porter_stemmed):\n",
    "        print(f\"'{word}'\", end=\"  \")\n",
    "        if (i + 1) % 12 == 0:\n",
    "            print()\n",
    "    print()\n",
    "    \n",
    "    print(f\"\\nğŸ“ Snowball Stemmed words:\")\n",
    "    for i, word in enumerate(snowball_stemmed):\n",
    "        print(f\"'{word}'\", end=\"  \")\n",
    "        if (i + 1) % 12 == 0:\n",
    "            print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acded3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ LEMMATIZATION RESULTS:\n",
      "============================================================\n",
      "ğŸ“Š Words processed: 126\n",
      "\n",
      "ğŸ” Comparison of original vs lemmatized words (first 20):\n",
      "Original        Lemmatized      Same?   \n",
      "--------------------------------------\n",
      "imagine         imagine         âœ“       \n",
      "each            each            âœ“       \n",
      "paragraph       paragraph       âœ“       \n",
      "as              a               âœ—       \n",
      "a               a               âœ“       \n",
      "sandwich        sandwich        âœ“       \n",
      "the             the             âœ“       \n",
      "real            real            âœ“       \n",
      "content         content         âœ“       \n",
      "of              of              âœ“       \n",
      "the             the             âœ“       \n",
      "sandwichthe     sandwichthe     âœ“       \n",
      "meat            meat            âœ“       \n",
      "or              or              âœ“       \n",
      "other           other           âœ“       \n",
      "fillingis       fillingis       âœ“       \n",
      "in              in              âœ“       \n",
      "the             the             âœ“       \n",
      "middle          middle          âœ“       \n",
      "it              it              âœ“       \n",
      "\n",
      "ğŸ“Š Words unchanged by lemmatization: 120/126 (95.2%)\n",
      "\n",
      "ğŸ“ Lemmatized words:\n",
      "'imagine'  'each'  'paragraph'  'a'  'a'  'sandwich'  'the'  'real'  'content'  'of'  'the'  'sandwichthe'  \n",
      "'meat'  'or'  'other'  'fillingis'  'in'  'the'  'middle'  'it'  'includes'  'all'  'the'  'evidence'  \n",
      "'you'  'need'  'to'  'make'  'the'  'point'  'but'  'it'  'get'  'kind'  'of'  'messy'  \n",
      "'to'  'eat'  'a'  'sandwich'  'without'  'any'  'bread'  'your'  'reader'  'don'  't'  'know'  \n",
      "'what'  'to'  'do'  'with'  'all'  'the'  'evidence'  'you'  've'  'given'  'them'  'so'  \n",
      "'the'  'top'  'slice'  'of'  'bread'  'the'  'first'  'sentence'  'of'  'the'  'paragraph'  'explains'  \n",
      "'the'  'topic'  'or'  'controlling'  'idea'  'of'  'the'  'paragraph'  'and'  'the'  'bottom'  'slice'  \n",
      "'the'  'last'  'sentence'  'of'  'the'  'paragraph'  'tell'  'the'  'reader'  'how'  'the'  'paragraph'  \n",
      "'relates'  'to'  'the'  'broader'  'argument'  'in'  'the'  'original'  'and'  'revised'  'paragraph'  'below'  \n",
      "'notice'  'how'  'a'  'topic'  'sentence'  'expressing'  'the'  'controlling'  'idea'  'tell'  'the'  'reader'  \n",
      "'the'  'point'  'of'  'all'  'the'  'evidence'  \n"
     ]
    }
   ],
   "source": [
    "# Perform lemmatization\n",
    "def perform_lemmatization(tokens):\n",
    "    \"\"\"Apply WordNet lemmatizer\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_words\n",
    "\n",
    "if original_text:\n",
    "    # Perform lemmatization\n",
    "    lemmatized_words = perform_lemmatization(cleaned_tokens)\n",
    "    \n",
    "    print(\"ğŸ¯ LEMMATIZATION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“Š Words processed: {len(cleaned_tokens)}\")\n",
    "    \n",
    "    # Compare original vs lemmatized\n",
    "    print(f\"\\nğŸ” Comparison of original vs lemmatized words (first 20):\")\n",
    "    print(f\"{'Original':<15} {'Lemmatized':<15} {'Same?':<8}\")\n",
    "    print(\"-\" * 38)\n",
    "    same_count = 0\n",
    "    for i in range(min(20, len(cleaned_tokens))):\n",
    "        original = cleaned_tokens[i]\n",
    "        lemmatized = lemmatized_words[i]\n",
    "        is_same = \"âœ“\" if original == lemmatized else \"âœ—\"\n",
    "        if original == lemmatized:\n",
    "            same_count += 1\n",
    "        print(f\"{original:<15} {lemmatized:<15} {is_same:<8}\")\n",
    "    \n",
    "    total_same = sum(1 for i in range(len(cleaned_tokens)) if cleaned_tokens[i] == lemmatized_words[i])\n",
    "    print(f\"\\nğŸ“Š Words unchanged by lemmatization: {total_same}/{len(cleaned_tokens)} ({total_same/len(cleaned_tokens)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Lemmatized words:\")\n",
    "    for i, word in enumerate(lemmatized_words):\n",
    "        print(f\"'{word}'\", end=\"  \")\n",
    "        if (i + 1) % 12 == 0:\n",
    "            print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59e440cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š COMPREHENSIVE ANALYSIS\n",
      "================================================================================\n",
      "ğŸ” Complete Comparison Table:\n",
      "   Original Porter_Stem Snowball_Stem  Lemmatized\n",
      "    imagine      imagin        imagin     imagine\n",
      "       each        each          each        each\n",
      "  paragraph   paragraph     paragraph   paragraph\n",
      "         as          as            as           a\n",
      "          a           a             a           a\n",
      "   sandwich    sandwich      sandwich    sandwich\n",
      "        the         the           the         the\n",
      "       real        real          real        real\n",
      "    content     content       content     content\n",
      "         of          of            of          of\n",
      "        the         the           the         the\n",
      "sandwichthe  sandwichth    sandwichth sandwichthe\n",
      "       meat        meat          meat        meat\n",
      "         or          or            or          or\n",
      "      other       other         other       other\n",
      "  fillingis    fillingi      fillingi   fillingis\n",
      "         in          in            in          in\n",
      "        the         the           the         the\n",
      "     middle       middl         middl      middle\n",
      "         it          it            it          it\n",
      "   includes      includ        includ    includes\n",
      "        all         all           all         all\n",
      "        the         the           the         the\n",
      "   evidence        evid          evid    evidence\n",
      "        you         you           you         you\n",
      "       need        need          need        need\n",
      "         to          to            to          to\n",
      "       make        make          make        make\n",
      "        the         the           the         the\n",
      "      point       point         point       point\n",
      "        but         but           but         but\n",
      "         it          it            it          it\n",
      "       gets         get           get         get\n",
      "       kind        kind          kind        kind\n",
      "         of          of            of          of\n",
      "      messy       messi         messi       messy\n",
      "         to          to            to          to\n",
      "        eat         eat           eat         eat\n",
      "          a           a             a           a\n",
      "   sandwich    sandwich      sandwich    sandwich\n",
      "    without     without       without     without\n",
      "        any         ani           ani         any\n",
      "      bread       bread         bread       bread\n",
      "       your        your          your        your\n",
      "    readers      reader        reader      reader\n",
      "        don         don           don         don\n",
      "          t           t             t           t\n",
      "       know        know          know        know\n",
      "       what        what          what        what\n",
      "         to          to            to          to\n",
      "         do          do            do          do\n",
      "       with        with          with        with\n",
      "        all         all           all         all\n",
      "        the         the           the         the\n",
      "   evidence        evid          evid    evidence\n",
      "        you         you           you         you\n",
      "         ve          ve            ve          ve\n",
      "      given       given         given       given\n",
      "       them        them          them        them\n",
      "         so          so            so          so\n",
      "        the         the           the         the\n",
      "        top         top           top         top\n",
      "      slice       slice         slice       slice\n",
      "         of          of            of          of\n",
      "      bread       bread         bread       bread\n",
      "        the         the           the         the\n",
      "      first       first         first       first\n",
      "   sentence     sentenc       sentenc    sentence\n",
      "         of          of            of          of\n",
      "        the         the           the         the\n",
      "  paragraph   paragraph     paragraph   paragraph\n",
      "   explains     explain       explain    explains\n",
      "        the         the           the         the\n",
      "      topic       topic         topic       topic\n",
      "         or          or            or          or\n",
      "controlling     control       control controlling\n",
      "       idea        idea          idea        idea\n",
      "         of          of            of          of\n",
      "        the         the           the         the\n",
      "  paragraph   paragraph     paragraph   paragraph\n",
      "        and         and           and         and\n",
      "        the         the           the         the\n",
      "     bottom      bottom        bottom      bottom\n",
      "      slice       slice         slice       slice\n",
      "        the         the           the         the\n",
      "       last        last          last        last\n",
      "   sentence     sentenc       sentenc    sentence\n",
      "         of          of            of          of\n",
      "        the         the           the         the\n",
      "  paragraph   paragraph     paragraph   paragraph\n",
      "      tells        tell          tell        tell\n",
      "        the         the           the         the\n",
      "     reader      reader        reader      reader\n",
      "        how         how           how         how\n",
      "        the         the           the         the\n",
      "  paragraph   paragraph     paragraph   paragraph\n",
      "    relates       relat         relat     relates\n",
      "         to          to            to          to\n",
      "        the         the           the         the\n",
      "    broader     broader       broader     broader\n",
      "   argument    argument      argument    argument\n",
      "         in          in            in          in\n",
      "        the         the           the         the\n",
      "   original      origin        origin    original\n",
      "        and         and           and         and\n",
      "    revised       revis         revis     revised\n",
      " paragraphs   paragraph     paragraph   paragraph\n",
      "      below       below         below       below\n",
      "     notice       notic         notic      notice\n",
      "        how         how           how         how\n",
      "          a           a             a           a\n",
      "      topic       topic         topic       topic\n",
      "   sentence     sentenc       sentenc    sentence\n",
      " expressing     express       express  expressing\n",
      "        the         the           the         the\n",
      "controlling     control       control controlling\n",
      "       idea        idea          idea        idea\n",
      "      tells        tell          tell        tell\n",
      "        the         the           the         the\n",
      "     reader      reader        reader      reader\n",
      "        the         the           the         the\n",
      "      point       point         point       point\n",
      "         of          of            of          of\n",
      "        all         all           all         all\n",
      "        the         the           the         the\n",
      "   evidence        evid          evid    evidence\n",
      "\n",
      "ğŸ“ˆ WORD FREQUENCY ANALYSIS:\n",
      "----------------------------------------\n",
      "ğŸ“Š Most frequent words (top 10):\n",
      "\n",
      "ğŸ”¤ Original words:\n",
      "   'the': 22\n",
      "   'of': 7\n",
      "   'paragraph': 5\n",
      "   'to': 4\n",
      "   'a': 3\n",
      "   'all': 3\n",
      "   'evidence': 3\n",
      "   'sentence': 3\n",
      "   'sandwich': 2\n",
      "   'or': 2\n",
      "\n",
      "âœ‚ï¸ Porter stemmed:\n",
      "   'the': 22\n",
      "   'of': 7\n",
      "   'paragraph': 6\n",
      "   'to': 4\n",
      "   'a': 3\n",
      "   'all': 3\n",
      "   'evid': 3\n",
      "   'reader': 3\n",
      "   'sentenc': 3\n",
      "   'sandwich': 2\n",
      "\n",
      "âœ‚ï¸ Snowball stemmed:\n",
      "   'the': 22\n",
      "   'of': 7\n",
      "   'paragraph': 6\n",
      "   'to': 4\n",
      "   'a': 3\n",
      "   'all': 3\n",
      "   'evid': 3\n",
      "   'reader': 3\n",
      "   'sentenc': 3\n",
      "   'sandwich': 2\n",
      "\n",
      "ğŸ¯ Lemmatized:\n",
      "   'the': 22\n",
      "   'of': 7\n",
      "   'paragraph': 6\n",
      "   'a': 4\n",
      "   'to': 4\n",
      "   'all': 3\n",
      "   'evidence': 3\n",
      "   'reader': 3\n",
      "   'sentence': 3\n",
      "   'sandwich': 2\n",
      "\n",
      "ğŸ“‰ VOCABULARY REDUCTION:\n",
      "------------------------------\n",
      "Original vocabulary size: 69\n",
      "Porter stemmed vocabulary: 67 (97.1% of original)\n",
      "Snowball stemmed vocabulary: 67 (97.1% of original)\n",
      "Lemmatized vocabulary: 66 (95.7% of original)\n",
      "\n",
      "ğŸ“‹ SUMMARY STATISTICS:\n",
      "--------------------\n",
      "âœ… Processing completed successfully!\n",
      "ğŸ“ Original text length: 715 characters\n",
      "ğŸ”¤ Total tokens extracted: 145\n",
      "ğŸ§¹ Clean tokens after processing: 126\n",
      "ğŸ“Š Unique words in original: 69\n",
      "âš¡ Tokens removed during cleaning: 19\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive comparison and analysis\n",
    "if original_text:\n",
    "    print(\"ğŸ“Š COMPREHENSIVE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create a comparison DataFrame\n",
    "    comparison_data = {\n",
    "        'Original': cleaned_tokens,\n",
    "        'Porter_Stem': porter_stemmed,\n",
    "        'Snowball_Stem': snowball_stemmed,\n",
    "        'Lemmatized': lemmatized_words\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame for better display\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"ğŸ” Complete Comparison Table:\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    print(f\"\\nğŸ“ˆ WORD FREQUENCY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Original word frequencies\n",
    "    original_freq = Counter(cleaned_tokens)\n",
    "    porter_freq = Counter(porter_stemmed)\n",
    "    snowball_freq = Counter(snowball_stemmed)\n",
    "    lemma_freq = Counter(lemmatized_words)\n",
    "    \n",
    "    print(f\"ğŸ“Š Most frequent words (top 10):\")\n",
    "    print(f\"\\nğŸ”¤ Original words:\")\n",
    "    for word, count in original_freq.most_common(10):\n",
    "        print(f\"   '{word}': {count}\")\n",
    "    \n",
    "    print(f\"\\nâœ‚ï¸ Porter stemmed:\")\n",
    "    for word, count in porter_freq.most_common(10):\n",
    "        print(f\"   '{word}': {count}\")\n",
    "    \n",
    "    print(f\"\\nâœ‚ï¸ Snowball stemmed:\")\n",
    "    for word, count in snowball_freq.most_common(10):\n",
    "        print(f\"   '{word}': {count}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Lemmatized:\")\n",
    "    for word, count in lemma_freq.most_common(10):\n",
    "        print(f\"   '{word}': {count}\")\n",
    "    \n",
    "    # Vocabulary reduction analysis\n",
    "    print(f\"\\nğŸ“‰ VOCABULARY REDUCTION:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Original vocabulary size: {len(set(cleaned_tokens))}\")\n",
    "    print(f\"Porter stemmed vocabulary: {len(set(porter_stemmed))} ({len(set(porter_stemmed))/len(set(cleaned_tokens))*100:.1f}% of original)\")\n",
    "    print(f\"Snowball stemmed vocabulary: {len(set(snowball_stemmed))} ({len(set(snowball_stemmed))/len(set(cleaned_tokens))*100:.1f}% of original)\")\n",
    "    print(f\"Lemmatized vocabulary: {len(set(lemmatized_words))} ({len(set(lemmatized_words))/len(set(cleaned_tokens))*100:.1f}% of original)\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nğŸ“‹ SUMMARY STATISTICS:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"âœ… Processing completed successfully!\")\n",
    "    print(f\"ğŸ“ Original text length: {len(original_text)} characters\")\n",
    "    print(f\"ğŸ”¤ Total tokens extracted: {len(tokens)}\")\n",
    "    print(f\"ğŸ§¹ Clean tokens after processing: {len(cleaned_tokens)}\")\n",
    "    print(f\"ğŸ“Š Unique words in original: {len(set(cleaned_tokens))}\")\n",
    "    print(f\"âš¡ Tokens removed during cleaning: {len(tokens) - len(cleaned_tokens)}\")\n",
    "else:\n",
    "    print(\"âŒ No text to analyze. Please check the input file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
