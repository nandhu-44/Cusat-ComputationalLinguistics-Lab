# -*- coding: utf-8 -*-
"""Playground.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWdosHh_Y_Dnn179rGrnAxf5s1Pdb3oN

# Q1
"""

input_paragraph = "She was in a hurry. Not the standard hurry when you're in a rush to get someplace, but a frantic hurry. The type of hurry where a few seconds could mean life or death. She raced down the road ignoring speed limits and weaving between cars. She was only a few minutes away when traffic came to a dead standstill on the road ahead."

# Program: Tokenization and Word Normalization
import nltk
import string
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Comment these out in exam if downloads already done
nltk.download('punkt_tab')
# nltk.download('punkt')
nltk.download('wordnet')
# nltk.download('omw-1.4')

text = input_paragraph

# 1. Tokenization
tokens = nltk.word_tokenize(text)

# 2. Lowercase + remove punctuation
clean_tokens = []
for tok in tokens:
    if tok not in string.punctuation:
        clean_tokens.append(tok.lower())

# 3. Stemming
stemmer = PorterStemmer()
stemmed = [stemmer.stem(w) for w in clean_tokens]

# 4. Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(w) for w in clean_tokens]

print("\nOriginal tokens :", tokens)
print("Clean tokens    :", clean_tokens)
print("Stemmed         :", stemmed)
print("Lemmatized      :", lemmatized)

# Program: Simple Tokenization and Stemming without NLTK
import string

def simple_tokenize(text: str):
    # split by whitespace, strip punctuation off ends
    tokens = []
    for word in text.split():
        word = word.strip(string.punctuation)
        if word:
            tokens.append(word)
    return tokens

def simple_stem(word: str):
    # very crude rule-based stemmer
    for suffix in ["ing", "ed", "ly", "es", "s"]:
        if word.endswith(suffix) and len(word) > len(suffix) + 2:
            return word[:-len(suffix)]
    return word

text = input_paragraph.lower()

tokens = simple_tokenize(text)
stems = [simple_stem(w) for w in tokens]

print("Tokens :", tokens)
print("Stems  :", stems)

# Program: Regex-based Tokenization and Normalization
import re
import string
from nltk.stem import PorterStemmer

# nltk.download('punkt')  # not required for regex, but OK to skip

text = input_paragraph

# \w+ = word characters, ' = apostrophe (for contractions)
tokens = re.findall(r"[A-Za-z']+", text)
tokens = [t.lower() for t in tokens]

stemmer = PorterStemmer()
stemmed = [stemmer.stem(w) for w in tokens]

print("Tokens :", tokens)
print("Stemmed:", stemmed)

"""# Q2

"""

import re
print(re.findall(r"\d", "Age is 21"))
print(re.findall(r"\d+", "Age is 21"))

"""# Q3"""

text_2 = "I can't believe it's working."

# Program: Rule-Based English Tokenizer
import re

def rule_based_tokenize(text):
    tokens = []
    words = text.split()

    for word in words:

        # --- Rule 1: Handle contractions (can't, won't) ---
        if word.lower().endswith("n't"):
            if word.lower() == "can't":
                tokens.extend(["ca", "n't"])
            elif word.lower() == "won't":
                tokens.extend(["wo", "n't"])
            else:
                tokens.extend([word[:-3], "n't"])
            continue

        # --- Rule 2: Handle 's contractions (John's) ---
        if "'s" in word:
            base = word.replace("'s", "")
            tokens.append(base)
            tokens.append("'s")
            continue

        # --- Rule 3: All caps abbreviations ---
        if re.fullmatch(r"[A-Z]+", word.strip(".,!?")):
            tokens.append(word.strip(".,!?"))
            continue

        # --- Rule 4: Hyphenated words ---
        if re.fullmatch(r"[A-Za-z]+-[A-Za-z]+", word):
            tokens.append(word)
            continue

        # --- Rule 5: General punctuation separation ---
        current = ""
        for ch in word:
            if ch in ".,!?;:":
                if current:
                    tokens.append(current)
                    current = ""
                tokens.append(ch)
            else:
                current += ch

        if current:
            tokens.append(current)

    return tokens

# -------- Main Program --------
text = text_2
result = rule_based_tokenize(text)
print("\nTokens:", result)

# Simple Rule-Based Tokenizer
import re

text = text_2

# Split by words and punctuation
tokens = re.findall(r"[A-Za-z]+(?:'[a-z]+)?|[.,!?]", text)

print("Tokens:", tokens)

# Tokenizer with contraction handling
import re

text = text_2

text = text.replace("can't", "ca n't")
text = text.replace("won't", "wo n't")

tokens = re.findall(r"\w+|[.,!?]", text)

print("Tokens:", tokens)

"""# Q4"""

# Program: Noisy Channel Model Spelling Corrector

from collections import Counter

# Dictionary with frequencies (can be any realistic numbers)
word_freq = {
    "the": 1000,
    "word": 500,
    "world": 450,
    "work": 400,
    "wood": 300,
    "lord": 200,
    "cord": 150,
    "bird": 120
}

V = list(word_freq.keys())

misspelled = "wrod"  # observed wrong spelling
candidates = ["word", "wood", "world"]  # possible corrections

print("Misspelled word:", misspelled)
print("Candidates:", candidates)

# Prior probabilities P(w)
total = sum(word_freq.values())
P_w = {w: word_freq[w] / total for w in V}

P_error = 0.05  # assume fixed channel probability for single edit

scores = {}

for w in candidates:
    # P(s | w) assumed constant for single-character error
    P_s_given_w = P_error
    score = P_s_given_w * P_w[w]
    scores[w] = score
    print(f"P(s|{w}) = {P_s_given_w:.3f}, P({w}) = {P_w[w]:.5f}, Score = {score:.7f}")

# Choose best candidate
best = max(scores, key=scores.get)

print("\nBest correction:", best)

# Noisy Channel using simple edit-distance-based P(s|w)

def edit_distance(a, b):
    # Levenshtein distance (DP)
    n, m = len(a), len(b)
    dp = [[0]*(m+1) for _ in range(n+1)]

    for i in range(n+1):
        dp[i][0] = i
    for j in range(m+1):
        dp[0][j] = j

    for i in range(1, n+1):
        for j in range(1, m+1):
            cost = 0 if a[i-1] == b[j-1] else 2
            dp[i][j] = min(
                dp[i-1][j] + 1,      # deletion
                dp[i][j-1] + 1,      # insertion
                dp[i-1][j-1] + cost  # substitution
            )
    return dp[n][m]

from math import exp

word_freq = {
    "word": 500,
    "wood": 300,
    "world": 450
}

misspelled = "wrod"
candidates = list(word_freq.keys())

total = sum(word_freq.values())
P_w = {w: word_freq[w] / total for w in word_freq}

scores = {}
for w in candidates:
    d = edit_distance(misspelled, w)
    # smaller distance → higher probability
    P_s_given_w = exp(-d)  # just a heuristic
    scores[w] = P_s_given_w * P_w[w]
    print(w, "distance:", d, "P(s|w):", round(P_s_given_w, 4), "P(w):", round(P_w[w], 4), "Score:", scores[w])

best = max(scores, key=scores.get)
print("\nBest correction:", best)

"""# Q5"""

# Naive Bayes using NLTK
import nltk
from nltk.tokenize import word_tokenize
from nltk import NaiveBayesClassifier

# nltk.download('punkt')

# Training data
train_data = [
    ("I love this movie", "pos"),
    ("This film is amazing", "pos"),
    ("I hate this movie", "neg"),
    ("This film is terrible", "neg")
]

# Feature extraction
def features(sentence):
    words = word_tokenize(sentence.lower())
    return {word: True for word in words}

train_features = [(features(text), label) for text, label in train_data]

# Train classifier
classifier = NaiveBayesClassifier.train(train_features)

# Test
test_sentence = "This movie is amazing"
result = classifier.classify(features(test_sentence))

print("Sentiment:", result)

"""# Q6"""

# !pip install stanza

# Program: POS Tagging in French using a Pretrained Model

import stanza

# Download models (run once; in lab you can comment this if already downloaded)
stanza.download('fr')
stanza.download('en')

# Create pipelines
nlp_fr = stanza.Pipeline('fr', processors='tokenize,pos', verbose=False)
nlp_en = stanza.Pipeline('en', processors='tokenize,pos', verbose=False)

french_sentences = [
    "Le chat mange une souris.",
    "Les enfants jouent dans le jardin.",
    "Marie et Paul sont très heureux aujourd'hui.",
    "Je vais à l'école en autobus.",
    "Nous aimons manger du fromage et boire du vin."
]

print("FRENCH POS TAGGING:\n")

for sent in french_sentences:
    print("Sentence:", sent)
    doc = nlp_fr(sent)
    for w in doc.sentences[0].words:
        # w.text = word, w.upos = universal POS, w.feats = morph features
        print(f"{w.text:12}  {w.upos:6}  {w.feats if w.feats else '-'}")
    print("-" * 40)

# For comparison: English
print("\nENGLISH EQUIVALENT TAG EXAMPLE:")
en_doc = nlp_en("The cat eats a mouse.")
for w in en_doc.sentences[0].words:
    print(f"{w.text:10} {w.upos}")

"""# Q7"""

# Program: Computing Translation Probabilities from a Parallel Corpus

from collections import defaultdict

# English–Malayalam parallel corpus (5 sentence pairs)
parallel_corpus = [
    ("the cat is sleeping",        "poocha urangunnund"),
    ("the dog is barking",         "naay kurakkunnund"),
    ("cat and dog are friends",    "poocha naay snehithamar"),
    ("we are friends",             "nammal snehitanmar"),
    ("the house is big",           "veedu valuthaan"),
    ("big cat in the house",       "valiya poocha veetil")
]

def compute_translation_probabilities(corpus):
    # count_ef[e][f] = Count(e, f)
    count_ef = defaultdict(lambda: defaultdict(int))
    count_e = defaultdict(int)  # Count(e)
    count_f = defaultdict(int)  # Count(f)

    for eng, mal in corpus:
        e_words = eng.split()
        f_words = mal.split()

        # Count English words
        for e in e_words:
            count_e[e] += 1

        # Count Malayalam words
        for f in f_words:
            count_f[f] += 1

        # Count co-occurrences
        for e in e_words:
            for f in f_words:
                count_ef[e][f] += 1

    # P(f|e)
    p_f_given_e = defaultdict(dict)
    for e in count_ef:
        for f in count_ef[e]:
            p_f_given_e[e][f] = count_ef[e][f] / count_e[e]

    # P(e|f)
    p_e_given_f = defaultdict(dict)
    for e in count_ef:
        for f in count_ef[e]:
            p_e_given_f[f][e] = count_ef[e][f] / count_f[f]

    return p_f_given_e, p_e_given_f, count_ef, count_e, count_f

def display_results(p_f_given_e, p_e_given_f, count_ef, count_e, count_f):
    print("PARALLEL CORPUS:")
    for i, (eng, mal) in enumerate(parallel_corpus, 1):
        print(f"{i}. English   : {eng}")
        print(f"   Malayalam : {mal}")
    print()

    print("WORD COUNTS (ENGLISH):")
    for e, c in sorted(count_e.items()):
        print(f"{e:10} -> {c}")
    print("\nWORD COUNTS (MALAYALAM):")
    for f, c in sorted(count_f.items()):
        print(f"{f:10} -> {c}")

    print("\nTRANSLATION PROBABILITIES P(Malayalam | English):")
    for e in sorted(p_f_given_e.keys()):
        print(f"'{e}' translates to:")
        for f, prob in sorted(p_f_given_e[e].items(), key=lambda x: -x[1]):
            print(f"   '{f}': {prob:.3f}  ( {count_ef[e][f]}/{count_e[e]} )")
        print()

    print("\nTRANSLATION PROBABILITIES P(English | Malayalam):")
    for f in sorted(p_e_given_f.keys()):
        print(f"'{f}' translates to:")
        for e, prob in sorted(p_e_given_f[f].items(), key=lambda x: -x[1]):
            print(f"   '{e}': {prob:.3f}  ( {count_ef[e][f]}/{count_f[f]} )")
        print()

def translate_word(word, p_trans, direction):
    if word not in p_trans:
        print(f"No translation found for '{word}'")
        return
    best_e_or_f, prob = max(p_trans[word].items(), key=lambda x: x[1])
    print(f"Most likely translation of '{word}' ({direction}): '{best_e_or_f}' with probability {prob:.3f}")

if __name__ == "__main__":
    p_f_given_e, p_e_given_f, count_ef, count_e, count_f = compute_translation_probabilities(parallel_corpus)
    display_results(p_f_given_e, p_e_given_f, count_ef, count_e, count_f)

    print("\nEXAMPLE TRANSLATIONS:")
    translate_word("cat", p_f_given_e, "English → Malayalam")
    translate_word("poocha", p_e_given_f, "Malayalam → English")

"""# Q8"""

# !pip install gensim

# Program: Word Clustering using Word Embeddings (Word2Vec + PCA)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from gensim.models import Word2Vec

def create_corpus():
    return [
        "machine learning is a subset of artificial intelligence",
        "deep learning is a type of machine learning",
        "neural networks are used in deep learning",
        "python is popular for machine learning",
        "data science involves statistics and programming",
        "natural language processing is part of artificial intelligence",
        "computer vision helps machines understand images",
        "supervised learning uses labeled data",
        "unsupervised learning finds patterns in data",
        "the cat sat on the mat",
        "dogs are loyal pets",
        "birds can fly in the sky",
        "fish swim in water",
        "trees provide oxygen and shade"
    ]

def train_word2vec(sentences, vector_size=50, window=3, min_count=1):
    tokenized = [s.lower().split() for s in sentences]
    model = Word2Vec(
        sentences=tokenized,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=4,
        epochs=100
    )
    return model

def cluster_and_plot(model):
    words = list(model.wv.index_to_key)
    vectors = np.array([model.wv[w] for w in words])

    # Reduce dimension to 2 using PCA
    pca = PCA(n_components=2)
    coords = pca.fit_transform(vectors)

    plt.figure(figsize=(10, 8))
    plt.scatter(coords[:, 0], coords[:, 1], alpha=0.7)

    for i, w in enumerate(words):
        plt.annotate(w, (coords[i, 0], coords[i, 1]), fontsize=9)

    plt.title("Word Clusters using Word2Vec + PCA")
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def print_similar_words(model, word):
    if word in model.wv:
        print(f"\nWords similar to '{word}':")
        for w, score in model.wv.most_similar(word, topn=5):
            print(f"{w:12} -> {score:.3f}")
    else:
        print(f"'{word}' not in vocabulary")

if __name__ == "__main__":
    corpus = create_corpus()
    model = train_word2vec(corpus)

    print("Vocabulary size:", len(model.wv))
    print_similar_words(model, "learning")
    print_similar_words(model, "python")
    print_similar_words(model, "cat")

    cluster_and_plot(model)

from gensim.models import Word2Vec

sentences = [
    "machine learning is powerful",
    "deep learning uses neural networks",
    "python is used in data science",
    "dogs are loyal animals",
    "cats are cute pets",
    "birds can fly in the sky"
]

tokenized = [s.lower().split() for s in sentences]

model = Word2Vec(sentences=tokenized, vector_size=50, window=3, min_count=1, epochs=100)

for word in ["learning", "python", "dogs", "cats"]:
    if word in model.wv:
        print(f"\nSimilar words to '{word}':")
        for w, score in model.wv.most_similar(word, topn=5):
            print(f"{w:10} -> {score:.3f}")

"""# Q9"""

# Program: FSA for English Plural Nouns Ending in 'y'

class PluralYFSA:
    def __init__(self):
        self.vowels = set("aeiou")
        self.consonants = set("bcdfghjklmnpqrstvwxyz")

    def accepts(self, word: str) -> bool:
        word = word.lower()

        # Need at least 3 chars: e.g. "ys" not enough
        if len(word) < 3:
            return False

        # Case 1: vowel + y + s  → boys, toys, days
        if word.endswith("ys"):
            # char before 'ys' is word[-3]
            if len(word) >= 3 and word[-3] in self.vowels:
                return True
            else:
                return False

        # Case 2: consonant + ies → ponies, ladies, babies
        if word.endswith("ies"):
            if len(word) >= 4 and word[-4] in self.consonants:
                return True
            else:
                return False

        # All other endings rejected
        return False

# ---------- Test ----------
if __name__ == "__main__":
    fsa = PluralYFSA()
    tests = [
        "boys", "toys", "days", "keys",
        "ponies", "skies", "puppies", "ladies",
        "boies", "toies", "ponys", "skys", "puppys",
        "boy", "pony", "cat", "cats"
    ]
    for w in tests:
        print(f"{w:10} ->", "ACCEPT" if fsa.accepts(w) else "REJECT")

def is_valid_plural_y(word: str) -> bool:
    vowels = set("aeiou")
    word = word.lower()

    if len(word) < 3:
        return False

    if word.endswith("ys"):
        return len(word) >= 3 and word[-3] in vowels

    if word.endswith("ies"):
        return len(word) >= 4 and word[-4] not in vowels

    return False

tests = ["boys", "ponies", "ponys", "boies", "days", "keys", "skies"]
for w in tests:
    print(w, "->", "ACCEPT" if is_valid_plural_y(w) else "REJECT")
    
    
"""
/kk - Digit and Phone Number Extraction using Regex & Rule-based English Tokenizer
/sss - Sentiment Analysis using Naive Bayes
/ppc - Translation Probabilities from Parallel Corpus
/word - Word Clustering using Word Embeddings
/hindi - POS Tagging in Hindi using Stanza
/fsa - FSA for English Plural Nouns Ending in 'y'
/token - Tokenization and Word Normalization
"""